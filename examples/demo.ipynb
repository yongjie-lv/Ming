{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Setup & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoProcessor, GenerationConfig\n",
    "from modeling_bailingmm import BailingMMNativeForConditionalGeneration\n",
    "\n",
    "# Load pre-trained model with optimized settings\n",
    "model = BailingMMNativeForConditionalGeneration.from_pretrained(\n",
    "    \"inclusionAI/Ming-Lite-Omni\",\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    "    low_cpu_mem_usage=True       # Minimize CPU memory during loading\n",
    ").to(\"cuda\")                     # Run on GPU\n",
    "\n",
    "# Initialize processor for handling multimodal inputs\n",
    "assets_path = YOUR_ASSETS_PATH   # Set your media directory path\n",
    "processor = AutoProcessor.from_pretrained(\"inclusionAI/Ming-Lite-Omni\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¬ Text QA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# é¹¦é¹‰æ˜¯ä¸€ç§éå¸¸èªæ˜å’Œç¤¾äº¤æ€§å¼ºçš„é¸Ÿç±»ï¼Œå®ƒä»¬çš„ç”Ÿæ´»ä¹ æ€§éå¸¸ä¸°å¯Œå’Œæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºé¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š\n",
    "# ### 1. **æ –æ¯åœ°**\n",
    "# é¹¦é¹‰ä¸»è¦åˆ†å¸ƒåœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºï¼ŒåŒ…æ‹¬éæ´²ã€äºšæ´²ã€æ¾³å¤§åˆ©äºšå’Œå—ç¾æ´²ã€‚å®ƒä»¬é€šå¸¸ç”Ÿæ´»åœ¨æ£®æ—ã€è‰åŸã€æ²™æ¼ å’ŒåŸå¸‚ç¯å¢ƒä¸­ã€‚ä¸åŒç§ç±»çš„é¹¦é¹‰å¯¹æ –æ¯åœ°çš„è¦æ±‚æœ‰æ‰€ä¸åŒï¼Œä½†å¤§å¤šæ•°é¹¦é¹‰å–œæ¬¢æœ‰ä¸°å¯Œæ¤è¢«å’Œæ°´æºçš„åœ°æ–¹ã€‚\n",
    "# ### 2. **é¥®é£Ÿ**\n",
    "# é¹¦é¹‰æ˜¯æ‚é£Ÿæ€§åŠ¨ç‰©ï¼Œå®ƒä»¬çš„é¥®é£Ÿéå¸¸å¤šæ ·åŒ–ã€‚å®ƒä»¬çš„é£Ÿç‰©åŒ…æ‹¬ç§å­ã€åšæœã€æ°´æœã€è”¬èœã€èŠ±èœœå’Œæ˜†è™«ã€‚é¹¦é¹‰çš„å–™éå¸¸å¼ºå£®ï¼Œèƒ½å¤Ÿè½»æ¾åœ°æ‰“å¼€åšç¡¬çš„æœå£³å’Œåšæœã€‚ä¸€äº›é¹¦é¹‰è¿˜ä¼šåƒæ³¥åœŸæˆ–æ²™å­ï¼Œä»¥å¸®åŠ©æ¶ˆåŒ–å’Œè¡¥å……çŸ¿ç‰©è´¨ã€‚\n",
    "# ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ–¼ï¸ Image QA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image qa\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": os.path.join(assets_path, \"flowers.jpg\")},\n",
    "            {\"type\": \"text\", \"text\": \"What kind of flower is this?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# The flowers in this image are forget-me-nots. These delicate blooms are known for their small, five-petaled flowers that come in various shades of blue, pink, and white. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” Chain-of-Thought Reasoning\n",
    "**To enable thinking before response, adding the following system prompt before your question:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = \"SYSTEM: You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in <thinking>...</thinking> tags, then the final answer enclosed in <answer>...</answer> tags. The critical answer or key result should be placed within \\\\boxed{}.\\n\"\n",
    "# And your input message should be like this:\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": os.path.join(assets_path, \"reasoning.png\")},\n",
    "            {\"type\": \"text\", \"text\": cot_prompt + \"In the rectangle $A B C D$ pictured, $M_{1}$ is the midpoint of $D C, M_{2}$ the midpoint of $A M_{1}, M_{3}$ the midpoint of $B M_{2}$ and $M_{4}$ the midpoint of $C M_{3}$. Determine the ratio of the area of the quadrilateral $M_{1} M_{2} M_{3} M_{4}$ to the area of the rectangle $A B C D$.\\nChoices:\\n(A) $\\frac{7}{16}$\\n(B) $\\frac{3}{16}$\\n(C) $\\frac{7}{32}$\\n(D) $\\frac{9}{32}$\\n(E) $\\frac{1}{5}$\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "# \\<think\\>\\nOkay, so I have this problem about a rectangle ABCD ... (thinking process omitted) ... So, the correct answer is C.\\n\\</think\\>\\n\\<answer\\>\\\\boxed{C}\\</answer\\>\\n\\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¥ Video QA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video qa\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": os.path.join(assets_path, \"yoga.mp4\")},\n",
    "            {\"type\": \"text\", \"text\": \"What is the woman doing?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# The image shows a woman performing a yoga pose on a rooftop. She's in a dynamic yoga pose, with her arms and legs extended in various positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¬ Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-turn chat\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"ASSISTANT\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"åŒ—äº¬\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"å®ƒçš„å åœ°é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿæœ‰å¤šå°‘å¸¸ä½äººå£ï¼Ÿ\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# Output:\n",
    "\n",
    "# åŒ—äº¬å¸‚çš„æ€»é¢ç§¯çº¦ä¸º16,410.54å¹³æ–¹å…¬é‡Œï¼Œå¸¸ä½äººå£çº¦ä¸º21,542,000äººã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Format inputs using chat template\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "# 2. Extract vision/audio data\n",
    "image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)\n",
    "\n",
    "# 3. Prepare tensor inputs\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    audios=audio_inputs,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "for k in inputs.keys():\n",
    "    if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "        inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "\n",
    "# 4. Configure generation\n",
    "generation_config = GenerationConfig.from_dict({'no_repeat_ngram_size': 10})\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    eos_token_id=processor.gen_terminator,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "# 5. Decode output\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”Š Audio Tasks\n",
    "\n",
    "For detailed usage for ASR, SpeechQA, and TTS tasks, please refer to `test_audio_tasks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\"},\n",
    "            {\"type\": \"audio\", \"audio\": 'data/wavs/BAC009S0915W0292.wav'},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# we use whisper encoder for ASR task, so need modify code above\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    audios=audio_inputs,\n",
    "    return_tensors=\"pt\",\n",
    "    audio_kwargs={'use_whisper_encoder': True}\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    eos_token_id=processor.gen_terminator,\n",
    "    generation_config=generation_config,\n",
    "    use_whisper_encoder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech2speech\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": 'data/wavs/speechQA_sample.wav'},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "generation_config = GenerationConfig.from_dict({\n",
    "    'output_hidden_states': True,\n",
    "    'return_dict_in_generate': True,\n",
    "    'no_repeat_ngram_size': 10}\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    eos_token_id=processor.gen_terminator,\n",
    "    generation_config=generation_config,\n",
    "    use_whisper_encoder=False\n",
    ")\n",
    "\n",
    "generated_ids = outputs.sequences\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# speechQA result\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "# for TTS\n",
    "from modeling_bailing_talker import AudioDetokenizer\n",
    "\n",
    "model_name_or_path = model.config._name_or_path\n",
    "audio_detokenizer = AudioDetokenizer(\n",
    "    f'{model_name_or_path}/talker/audio_detokenizer.yaml',\n",
    "    flow_model_path=f'{model_name_or_path}/talker/flow.pt',\n",
    "    hifigan_model_path=f'{model_name_or_path}/talker/hift.pt'\n",
    ")\n",
    "spk_input = torch.load('data/spks/luna.pt')\n",
    "thinker_reply_part = outputs.hidden_states[0][0] + outputs.hidden_states[0][-1]\n",
    "# Setting thinker_reply_part to None allows the talker to operate as a standalone TTS model, independent of the language model.\n",
    "audio_tokens = model.talker.omni_audio_generation(\n",
    "    output_text, \n",
    "    thinker_reply_part=thinker_reply_part, **spk_input)\n",
    "waveform = audio_detokenizer.token2wav(audio_tokens, save_path='out.wav', **spk_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¨ Image Generation & Editing\n",
    "\n",
    "Ming-omni natively supports image generation and image editing. To use this function, you only need to add the corresponding parameters in the generate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generation mode currently limits the range of input pixels.\n",
    "gen_input_pixels = 451584\n",
    "processor.max_pixels = gen_input_pixels\n",
    "processor.min_pixels = gen_input_pixels\n",
    "\n",
    "def generate(messages, processor, model, **image_gen_param):\n",
    "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        audios=audio_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    for k in inputs.keys():\n",
    "        if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "            inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "    \n",
    "    print(image_gen_param)\n",
    "    image = model.generate(\n",
    "        **inputs,\n",
    "        image_gen=True,\n",
    "        **image_gen_param,\n",
    "    )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-to-image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Draw a girl with short hair.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "image = generate(\n",
    "   messages=messages, processor=processor, model=model, \n",
    "   image_gen_cfg=6.0, image_gen_steps=20, image_gen_width=480, image_gen_height=544\n",
    ")\n",
    "image.save(\"./t2i.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"HUMAN\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"samples/cake.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"add a candle on top of the cake\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "image = generate(\n",
    "   messages=messages, processor=processor, model=model, \n",
    "   image_gen_cfg=6.0, image_gen_steps=20, image_gen_width=512, image_gen_height=512\n",
    ")\n",
    "image.save(\"./edit.jpg\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
