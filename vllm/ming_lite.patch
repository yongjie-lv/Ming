From a37daf9e962226463680170ebd98178502ff1a28 Mon Sep 17 00:00:00 2001
From: "wanpengfei.wpf@antgroup.com" <wanpengfei.wpf>
Date: Mon, 7 Jul 2025 20:45:48 +0800
Subject: [PATCH] feat(Ming): support Ming-Lite-v1.5 1.suuport ming-lite-v1.5
 text/image/video/audio full modal scenarios 2.support
 image_generate/image_edit scenarios 3.support return hidden_states 4.support
 ming-lite-v1.5-fp8 5.fix audio/video accuracy 6.add test demo

---
 test/test_bailingmm_v1.5.py                   | 103 ++
 vllm/engine/llm_engine.py                     |  12 +-
 .../compressed_tensors_moe.py                 |   2 +
 .../quantization/compressed_tensors/utils.py  |   2 +-
 .../model_executor/layers/rotary_embedding.py | 181 +++-
 vllm/model_executor/models/bailing_moe.py     | 645 ++++++++++++
 vllm/model_executor/models/bailingmm.py       | 980 ++++++++++++++++++
 vllm/model_executor/models/registry.py        |   2 +
 vllm/outputs.py                               |  20 +-
 vllm/sampling_params.py                       |   3 +
 vllm/sequence.py                              |   2 +
 vllm/transformers_utils/config.py             |  16 +-
 vllm/transformers_utils/configs/__init__.py   |   2 +
 .../transformers_utils/configs/bailing_moe.py |  76 ++
 vllm/v1/executor/multiproc_executor.py        |   7 +-
 vllm/worker/model_runner.py                   |  32 +-
 16 files changed, 2072 insertions(+), 13 deletions(-)
 create mode 100644 test/test_bailingmm_v1.5.py
 create mode 100644 vllm/model_executor/models/bailing_moe.py
 create mode 100644 vllm/model_executor/models/bailingmm.py
 create mode 100644 vllm/transformers_utils/configs/bailing_moe.py

diff --git a/test/test_bailingmm_v1.5.py b/test/test_bailingmm_v1.5.py
new file mode 100644
index 000000000..8e0540c24
--- /dev/null
+++ b/test/test_bailingmm_v1.5.py
@@ -0,0 +1,103 @@
+#!/usr/bin/python
+#****************************************************************#
+# ScriptName: test_bailingmm_v1.5.py
+# Author: $SHTERM_REAL_USER@alibaba-inc.com
+# Create Date: 2025-07-08 11:26
+# Modify Author: $SHTERM_REAL_USER@alibaba-inc.com
+# Modify Date: 2025-07-17 10:18
+# Function: 
+#***************************************************************#
+
+import os
+os.environ["VLLM_USE_V1"] = "0"
+
+
+import vllm
+from vllm import LLM, SamplingParams
+from vllm.inputs import TextPrompt as LLMInputs
+from transformers import AutoTokenizer, AutoProcessor, AutoConfig
+import sys
+import os
+import torch
+
+os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'
+bl_mm = "/hetero_infer_new/serina.wzq/bailingv4_moe_lite/"
+
+sys.path.insert(0, bl_mm)
+
+sampling_params = SamplingParams(temperature=0, max_tokens=512)
+
+if __name__=="__main__":
+    llm = LLM(model=bl_mm, trust_remote_code=True, enforce_eager=True, disable_custom_all_reduce=True, tensor_parallel_size=1, limit_mm_per_prompt={"image": 10})
+
+    tokenizer = AutoTokenizer.from_pretrained(bl_mm, trust_remote_code=True)
+    processor = AutoProcessor.from_pretrained(bl_mm, trust_remote_code=True)
+
+
+
+    vision_path = "/hetero_infer_new/LLMs/assets/"
+    messages = [
+        {
+            "role": "HUMAN",
+            "content": [
+                {"type": "image", "image": os.path.join(vision_path, "flowers.jpg")},
+                {"type": "text", "text": "这个里面的是什么东西，什么颜色?"},
+            ],
+        },
+    ]
+    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, use_system=True)
+    image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)
+
+    message_2 = [
+        {
+            "role": "HUMAN",
+            "content": [
+                {"type": "image", "image": "/root/workspace/0.jpg"},
+                {"type": "image", "image": "/root/workspace/1.jpg"},
+                {"type": "image", "image": os.path.join(vision_path, "flowers.jpg")},
+                {"type": "text", "text": "Question: Which option describe the object relationship in the image correctly?\nOptions:\nA. The suitcase is on the book.\nB. The suitcase is beneath the cat.\nC. The suitcase is beneath the bed.\nD. The suitcase is beneath the book.\nPlease select the correct answer from the options above."},
+            ],
+        },
+    ]
+    text_i2 = processor.apply_chat_template(message_2, tokenize=False, add_generation_prompt=True, use_system=True)
+    image_inputs_i2, video_inputs_i2, audio_inputs_i2 = processor.process_vision_info(message_2)
+
+    messages = [
+        {
+            "role": "HUMAN",
+            "content": [
+                {"type": "video", "video": os.path.join(vision_path, "yoga.mp4"), 'min_pixels': 100352, 'max_pixels': 602112, "sample": "uniform"},
+                {"type": "text", "text": "What is the woman doing?"},
+            ],
+        },
+    ]
+
+    text_1 = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, use_system=True)
+    image_inputs_1, video_inputs_1, audio_inputs_1 = processor.process_vision_info(messages)
+
+    messages = [
+       {
+           "role": "HUMAN",
+           "content": [
+               {"type": "text", "text": "Please recognize the language of this speech and transcribe it. Format: oral."},
+               {"type": "audio", "audio": '/hetero_infer_new/serina.wzq/bailingv4_moe_lite/data/wavs/BAC009S0915W0292.wav'},
+           ],
+        },
+   ]
+
+    text_2 = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, use_system=True)
+
+    image_inputs_2, video_inputs_2, audio_inputs_2 = processor.process_vision_info(messages)
+
+
+
+    requests = [LLMInputs({ "prompt": text, "multi_modal_data": {"image": image_inputs} }),
+                LLMInputs({ "prompt": text, "multi_modal_data": {"image": image_inputs} }),
+                LLMInputs({ "prompt": text_i2, "multi_modal_data": {"image": image_inputs_i2} }),
+                LLMInputs({ "prompt": text_1, "multi_modal_data": {"video": video_inputs_1} }),
+                LLMInputs({ "prompt": text_2, "multi_modal_data": {"audio": audio_inputs_2} }),
+    ]
+    print(requests)
+    sampling_params = SamplingParams(temperature=0, max_tokens=512)
+    outputs = llm.generate(requests, sampling_params)
+    print(outputs)
diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
index c23530990..6cb3b1a22 100644
--- a/vllm/engine/llm_engine.py
+++ b/vllm/engine/llm_engine.py
@@ -1114,6 +1114,14 @@ class LLMEngine:
                         else:
                             seq_group.metrics.model_execute_time = (
                                 o.model_execute_time)
+                        if(hasattr(o, "prefill_hidden_states")):
+                            prefill_hidden_states = o.prefill_hidden_states
+                        else:
+                            prefill_hidden_states = None
+                        if(hasattr(o, "hidden_states")):
+                            hidden_states = o.hidden_states
+                        else:
+                            hidden_states = None
 
             if self.model_config.runner_type == "pooling":
                 self._process_sequence_group_outputs(seq_group, output)
@@ -1137,7 +1145,9 @@ class LLMEngine:
             request_output = RequestOutputFactory.create(
                 seq_group,
                 self.seq_id_to_seq_group,
-                use_cache=self.use_cached_outputs)
+                use_cache=self.use_cached_outputs,
+                hidden_states=hidden_states,
+                prefill_hidden_states=prefill_hidden_states)
             if request_output:
                 ctx.request_outputs.append(request_output)
 
diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
index 721e36af2..fbf76c7c3 100644
--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
@@ -289,6 +289,7 @@ class CompressedTensorsW8A8Fp8MoEMethod(CompressedTensorsMoEMethod):
         e_score_correction_bias: Optional[torch.Tensor] = None,
         apply_router_weight_on_input: bool = False,
         activation: str = "silu",
+        router_scaling: Optional[float] = None,
     ) -> torch.Tensor:
 
         topk_weights, topk_ids = FusedMoE.select_experts(
@@ -507,6 +508,7 @@ class CompressedTensorsW8A8Fp8MoECutlassMethod(CompressedTensorsMoEMethod):
         e_score_correction_bias: Optional[torch.Tensor] = None,
         apply_router_weight_on_input: bool = False,
         activation: str = "silu",
+        router_scaling: Optional[float] = None,
     ) -> torch.Tensor:
 
         assert activation == "silu"
diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/utils.py b/vllm/model_executor/layers/quantization/compressed_tensors/utils.py
index 85ae1d5cb..b5ba95296 100644
--- a/vllm/model_executor/layers/quantization/compressed_tensors/utils.py
+++ b/vllm/model_executor/layers/quantization/compressed_tensors/utils.py
@@ -173,7 +173,7 @@ def _match_fused_layer(
         layer_name: str, target_layers: Iterable[str],
         fused_mapping: Mapping[str, List[str]]) -> Optional[str]:
     """
-    Match a fused layer name to its corresponding individual layer in 
+    Match a fused layer name to its corresponding individual layer in
     target_layers. Returns first value in fused_mapping which matches targets
 
     Implements an "all" matching strategy where a fused layer matches iff
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index c5970c71c..38028355e 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -1014,7 +1014,6 @@ class MRotaryEmbedding(RotaryEmbedding):
         use_audio_in_video: bool = False,
     ) -> Tuple[List[List[int]], int]:
         """Get mrope input positions and delta value."""
-
         image_grid_thw = [] if image_grid_thw is None else image_grid_thw
         video_grid_thw = [] if video_grid_thw is None else video_grid_thw
         second_per_grid_ts = [] if second_per_grid_ts is None else \
@@ -1048,7 +1047,7 @@ class MRotaryEmbedding(RotaryEmbedding):
         audio_feature_lengths: Optional[torch.Tensor] = None,
         use_audio_in_video: bool = False,
     ) -> Tuple[torch.Tensor, int]:
-        from vllm.transformers_utils.config import thinker_uses_mrope
+        from vllm.transformers_utils.config import thinker_uses_mrope, llm_uses_mrope
         if thinker_uses_mrope(hf_config):
             return cls._omni_get_input_positions_tensor(
                 input_tokens=input_tokens,
@@ -1061,6 +1060,18 @@ class MRotaryEmbedding(RotaryEmbedding):
                 audio_feature_lengths=audio_feature_lengths,
                 use_audio_in_video=use_audio_in_video,
             )
+        elif llm_uses_mrope(hf_config):
+            return cls._bailing_3drope_get_input_positions_tensor(
+                input_tokens=input_tokens,
+                hf_config=hf_config,
+                image_grid_thw=image_grid_thw,
+                video_grid_thw=video_grid_thw,
+                second_per_grid_ts=second_per_grid_ts,
+                context_len=context_len,
+                seq_len=seq_len,
+                audio_feature_lengths=audio_feature_lengths,
+                use_audio_in_video=use_audio_in_video,
+            )
         else:
             return cls._vl_get_input_positions_tensor(
                 input_tokens=input_tokens,
@@ -1356,6 +1367,172 @@ class MRotaryEmbedding(RotaryEmbedding):
 
         return llm_positions, mrope_position_delta
 
+    @classmethod
+    def _bailing_3drope_get_input_positions_tensor(
+        cls,
+        input_tokens: List[int],
+        hf_config: PretrainedConfig,
+        image_grid_thw: Union[List[List[int]], torch.Tensor],
+        video_grid_thw: Union[List[List[int]], torch.Tensor],
+        second_per_grid_ts: Optional[List[float]] = None,
+        context_len: int = 0,
+        seq_len: Optional[int] = None,
+        audio_feature_lengths: Optional[torch.Tensor] = None,
+        use_audio_in_video: bool = False,
+    ) -> torch.Tensor:
+        second_per_grid_ts = None # FIXME(serina.wzq): This variable is unused currently
+        attention_mask = None
+        use_abs_time_pos = second_per_grid_ts is not None
+        spatial_merge_size = hf_config.vision_config.spatial_merge_size
+        tokens_per_second = hf_config.vision_config.tokens_per_second
+        llm_config = hf_config.llm_config
+        audio_patch_token_id = llm_config.audio_patch_token
+        image_patch_id = llm_config.image_patch_token
+        video_patch_id = llm_config.image_patch_token
+        audio_start_token_id = llm_config.audio_start_token
+        audio_end_token_id = llm_config.audio_end_token
+        image_start_token_id = llm_config.image_start_token
+        image_end_token_id = llm_config.image_end_token
+        video_start_token_id = llm_config.video_start_token
+        video_end_token_id = llm_config.video_end_token
+        input_ids = torch.tensor(input_tokens)
+
+        mrope_position_deltas = []
+        if image_grid_thw is not None or video_grid_thw is not None:
+            total_input_ids = input_ids
+            position_ids = torch.ones(
+                3,
+                input_ids.shape[0],
+                dtype=input_ids.dtype,
+                device=input_ids.device,
+            )
+
+            image_index, video_index = 0, 0
+            image_nums, video_nums = 0, 0
+            if image_grid_thw is not None:
+                vision_start_indices = torch.argwhere(
+                    input_ids == image_start_token_id
+                ).squeeze(1)
+                vision_tokens = input_ids[vision_start_indices + 1]
+                image_nums = (vision_tokens == image_patch_id).sum()
+            if video_grid_thw is not None:
+                vision_start_indices = torch.argwhere(
+                    input_ids == video_start_token_id
+                ).squeeze(1)
+                vision_tokens = input_ids[vision_start_indices + 1]
+                video_nums = (vision_tokens == video_patch_id).sum()
+
+            input_tokens = input_ids.tolist()
+            llm_pos_ids_list: list = []
+            st = 0
+            remain_images, remain_videos = image_nums, video_nums
+            for _ in range(image_nums + video_nums):
+                if image_patch_id in input_tokens and remain_images > 0:
+                    ed_image = input_tokens.index(image_patch_id, st)
+                else:
+                    ed_image = len(input_tokens) + 1
+                if video_patch_id in input_tokens and remain_videos > 0:
+                    ed_video = input_tokens.index(video_patch_id, st)
+                else:
+                    ed_video = len(input_tokens) + 1
+                if ed_image < ed_video:
+                    t, h, w = (
+                        image_grid_thw[image_index][0],
+                        image_grid_thw[image_index][1],
+                        image_grid_thw[image_index][2],
+                    )
+                    second_per_grid_t = 0
+                    image_index += 1
+                    remain_images -= 1
+                    ed = ed_image
+                else:
+                    t, h, w = (
+                        video_grid_thw[video_index][0],
+                        video_grid_thw[video_index][1],
+                        video_grid_thw[video_index][2],
+                    )
+
+                    if second_per_grid_ts is not None:
+                        second_per_grid_t = second_per_grid_ts[video_index]
+                    else:
+                        second_per_grid_t = 1.0
+
+                    video_index += 1
+                    remain_videos -= 1
+                    ed = ed_video
+                llm_grid_t, llm_grid_h, llm_grid_w = (
+                    t.item(),
+                    h.item() // spatial_merge_size,
+                    w.item() // spatial_merge_size,
+                )
+                text_len = ed - st
+
+
+                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+                llm_pos_ids_list.append(
+                    torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
+                )
+
+                range_tensor = torch.arange(llm_grid_t).view(-1, 1)
+                expanded_range = range_tensor.expand(-1, llm_grid_h * llm_grid_w)
+                if use_abs_time_pos:
+                    time_tensor = expanded_range * second_per_grid_t * tokens_per_second
+                    time_tensor_long = time_tensor.long()
+                else:
+                    time_tensor_long = expanded_range.long()
+                t_index = time_tensor_long.flatten()
+
+                h_index = (
+                    torch.arange(llm_grid_h)
+                    .view(1, -1, 1)
+                    .expand(llm_grid_t, -1, llm_grid_w)
+                    .flatten()
+                )
+                w_index = (
+                    torch.arange(llm_grid_w)
+                    .view(1, 1, -1)
+                    .expand(llm_grid_t, llm_grid_h, -1)
+                    .flatten()
+                )
+
+                llm_pos_ids_list.append(
+                    torch.stack([t_index, h_index, w_index]) + text_len + st_idx
+                )
+                st = ed + llm_grid_t * llm_grid_h * llm_grid_w
+
+            if st < len(input_tokens):
+                st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+                text_len = len(input_tokens) - st
+                llm_pos_ids_list.append(
+                    torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
+                )
+            llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
+            position_ids = llm_positions.to(position_ids.device)
+            mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids))
+            mrope_position_deltas = torch.tensor(
+                mrope_position_deltas, device=input_ids.device
+            ).unsqueeze(1)
+        else:
+            if attention_mask is not None:
+                position_ids = attention_mask.long().cumsum(-1) - 1
+                position_ids.masked_fill_(attention_mask == 0, 1)
+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)
+                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
+                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
+            else:
+                position_ids = (
+                    torch.arange(input_ids.shape[1], device=input_ids.device)
+                    .view(1, 1, -1)
+                    .expand(3, input_ids.shape[0], -1)
+                )
+
+                mrope_position_deltas = torch.zeros(
+                    [input_ids.shape[0], 1],
+                    device=input_ids.device,
+                    dtype=input_ids.dtype,
+                )
+        return position_ids, mrope_position_deltas
+
     @staticmethod
     def _get_llm_pos_ids_for_vision(
         start_idx: int,
diff --git a/vllm/model_executor/models/bailing_moe.py b/vllm/model_executor/models/bailing_moe.py
new file mode 100644
index 000000000..684e24fc1
--- /dev/null
+++ b/vllm/model_executor/models/bailing_moe.py
@@ -0,0 +1,645 @@
+# coding=utf-8
+""" PyTorch Bailing model. """
+
+from typing import Iterable, List, Optional, Tuple, Union, Set
+
+import torch
+from torch import nn
+
+from vllm.model_executor.layers.activation import get_act_fn, SiluAndMul
+from vllm.attention import Attention, AttentionMetadata
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.fused_moe import fused_moe, FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                               MergedColumnParallelLinear,
+                                               ReplicatedLinear,
+                                               QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig)
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import Sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.distributed import (get_pp_group,
+                              get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.utils import set_weight_attrs
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.sequence import IntermediateTensors
+from vllm.transformers_utils.configs.bailing_moe import BailingMoeConfig
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.config import LoRAConfig
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .utils import (PPMissingLayer,
+                    is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory,
+                    make_layers,
+                    maybe_prefix)
+
+
+class BailingAttention(nn.Module):
+
+    def __init__(
+            self,
+            config: BailingMoeConfig,
+            cache_config: Optional[CacheConfig] = None,
+            quant_config: Optional[QuantizationConfig] = None,
+            prefix: str = "",
+    ):
+        super().__init__()
+        # hidden_states大小
+        self.hidden_size = config.hidden_size
+        # q头数
+        self.total_num_heads = config.num_attention_heads
+        # kv头数（mha下跟total_num_heads相等，mqa或gqa小于total_num_heads）
+        self.total_kv_heads = config.num_key_value_heads
+        # 并发数
+        tp_size = get_tensor_model_parallel_world_size()
+
+        # q 头或 kv头数要被tp整除
+        assert self.total_num_heads % tp_size == 0
+        assert self.total_kv_heads % tp_size == 0
+        assert self.total_num_heads >= self.total_kv_heads
+
+        # 当前gpu rank下的q头
+        self.num_heads = self.total_num_heads // tp_size
+        # 每个q头的维度
+        self.head_dim = config.head_dim or (self.hidden_size // self.total_num_heads)
+        # 当前gpu的q_size
+        self.q_size_per_rank = self.head_dim * self.num_heads
+
+        # 当前gpu rank下的kv头
+        self.num_kv_heads = self.total_kv_heads // tp_size
+        # 当前gpu的k/v维度
+        self.kv_size_per_rank = self.num_kv_heads * self.head_dim
+
+        self.scale = self.head_dim ** -0.5
+
+        self.query_key_value = QKVParallelLinear(
+            self.hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_kv_heads,
+            bias=(config.use_bias or config.use_qkv_bias),
+            quant_config=quant_config,
+            prefix=f"{prefix}.query_key_value",
+        )
+
+        self.dense = RowParallelLinear(self.total_num_heads * self.head_dim,
+                                       self.hidden_size,
+                                       bias=config.use_bias,
+                                       quant_config=quant_config,
+                                       prefix=f"{prefix}.dense",)
+
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scale,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              prefix=f"{prefix}.attn")
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=config.max_position_embeddings,
+            base=config.rope_theta,
+            is_neox_style=True,
+            rope_scaling=config.rope_scaling,
+        )
+
+    def forward(
+            self,
+            hidden_states: torch.Tensor,
+            position_ids: torch.Tensor,
+    ) -> torch.Tensor:
+
+        qkv, _ = self.query_key_value(hidden_states)
+        q, k, v = qkv.split(
+            [self.q_size_per_rank, self.kv_size_per_rank, self.kv_size_per_rank],
+            dim=-1
+        )
+
+
+        q, k = self.rotary_emb(position_ids, q, k)
+
+        context_layer = self.attn(
+            q,
+            k,
+            v,
+        )
+
+        attn_output, _ = self.dense(context_layer)
+        return attn_output
+
+
+class BailingMLP(nn.Module):
+
+    def __init__(
+            self,
+            intermediate_size: int,
+            config: BailingMoeConfig,
+            quant_config: Optional[QuantizationConfig] = None,
+            reduce_results: Optional[bool] = True,
+            prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            config.hidden_size, [intermediate_size] * 2,
+            bias=config.use_bias,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj",
+        )
+        self.down_proj = RowParallelLinear(
+            intermediate_size,
+            config.hidden_size,
+            bias=config.use_bias,
+            quant_config=quant_config,
+            reduce_results=reduce_results,
+            prefix=f"{prefix}.down_proj",
+        )
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        x, _ = self.gate_up_proj(x)
+        x = self.act_fn(x)
+        x, _ = self.down_proj(x)
+        return x
+
+class BailingMoE(nn.Module):
+
+    def __init__(
+            self,
+            intermediate_size: int,
+            config: BailingMoeConfig,
+            quant_config: Optional[QuantizationConfig] = None,
+            reduce_results: Optional[bool] = True,
+            prefix: str = "",
+    ):
+        super().__init__()
+
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.tp_rank = get_tensor_model_parallel_rank()
+        self.num_experts = config.num_experts
+        self.top_k = config.num_experts_per_tok
+        self.norm_expert_prob = config.norm_topk_prob
+        self.hidden_size = config.hidden_size
+        self.quant_config = quant_config
+        self.num_shared_experts = config.num_shared_experts
+        # Gate always runs at half / full precision for now.
+        self.gate = ReplicatedLinear(self.hidden_size,
+                                     self.num_experts,
+                                     bias=False,
+                                     quant_config=None)
+
+        self.experts = FusedMoE(
+            num_experts=self.num_experts,
+            top_k=self.top_k,
+            hidden_size=self.hidden_size,
+            intermediate_size=config.moe_intermediate_size,
+            reduce_results=False,
+            renormalize=self.norm_expert_prob,
+            quant_config=quant_config,
+            prefix=f"{prefix}.experts"
+        )
+
+        if self.num_shared_experts > 0:
+            intermediate_size = (config.moe_intermediate_size *
+                                 self.num_shared_experts)
+            self.shared_experts = BailingMLP(
+                intermediate_size=intermediate_size,
+                config=config,
+                quant_config=quant_config,
+                reduce_results=False,
+                prefix=f"{prefix}.shared_experts"
+            )
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        num_tokens, hidden_size = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_size)
+        if self.num_shared_experts > 0:
+            shared_output = self.shared_experts(hidden_states)
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.experts(
+            hidden_states=hidden_states, router_logits=router_logits
+        )
+
+        if self.num_shared_experts > 0:
+            final_hidden_states = final_hidden_states + shared_output
+
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+        return final_hidden_states.view(num_tokens, hidden_size)
+
+class BailingSparseMoe(nn.Module):
+
+    def __init__(
+            self,
+            intermediate_size: int,
+            config: BailingMoeConfig,
+            quant_config: Optional[QuantizationConfig] = None,
+            reduce_results: Optional[bool] = True,
+            prefix: str = "",
+    ):
+        super().__init__()
+
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.tp_rank = get_tensor_model_parallel_rank()
+        self.num_experts = config.num_experts
+        self.top_k = config.num_experts_per_tok
+        self.norm_expert_prob = config.norm_topk_prob
+        self.hidden_size = config.hidden_size
+        self.quant_config = quant_config
+        self.num_shared_experts = config.num_shared_experts
+        self.multi_gate = config.multi_gate
+
+        # Gate always runs at half / full precision for now.
+        self.gate = ReplicatedLinear(self.hidden_size,
+                                     self.num_experts,
+                                     bias=False,
+                                     quant_config=None)
+        if self.multi_gate:
+            self.image_gate = ReplicatedLinear(self.hidden_size, self.num_experts, bias=False, quant_config=None)
+            self.audio_gate = ReplicatedLinear(self.hidden_size, self.num_experts, bias=False, quant_config=None)
+        self.experts = FusedMoE(
+            num_experts=self.num_experts,
+            top_k=self.top_k,
+            hidden_size=self.hidden_size,
+            intermediate_size=config.moe_intermediate_size,
+            reduce_results=False,
+            renormalize=self.norm_expert_prob,
+            quant_config=quant_config,
+            prefix=f"{prefix}.experts"
+        )
+
+        if self.num_shared_experts > 0:
+            intermediate_size = (config.moe_intermediate_size *
+                                 self.num_shared_experts)
+            self.shared_experts = BailingMLP(
+                intermediate_size=intermediate_size,
+                config=config,
+                quant_config=quant_config,
+                reduce_results=False,
+                prefix=f"{prefix}.shared_experts"
+            )
+
+    def forward(self, hidden_states: torch.Tensor, intermediate_tensors: Optional[IntermediateTensors],) -> torch.Tensor:
+        multimodal_input = False
+        image_mask = None
+        audio_mask = None
+        if self.multi_gate and intermediate_tensors is not None and 'image_mask' in intermediate_tensors.tensors:
+            multimodal_input = True
+            image_mask = intermediate_tensors["image_mask"]
+            audio_mask = intermediate_tensors["audio_mask"]
+
+        num_tokens, hidden_size = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_size)
+        if self.num_shared_experts > 0:
+            shared_output = self.shared_experts(hidden_states)
+
+        # multigate: image, audio, ant text hidden_state use different gate
+        if self.multi_gate and multimodal_input:
+            router_logits = torch.zeros([num_tokens, self.num_experts], dtype=torch.bfloat16, device=hidden_states.device)
+            # init token mask
+            text_mask = torch.ones([num_tokens], dtype=torch.bool, device=hidden_states.device)
+            if image_mask is not None:
+                text_mask = ~image_mask
+            if audio_mask is not None:
+                text_mask = text_mask & (~audio_mask)
+            text_router_logits, _  = self.gate(hidden_states[text_mask])
+            router_logits[text_mask] = text_router_logits
+            if image_mask is not None:
+                image_router_logits, _ = self.image_gate(hidden_states[image_mask])
+                router_logits[image_mask] = image_router_logits
+            if audio_mask is not None:
+                audio_router_logits, _ = self.audio_gate(hidden_states[audio_mask])
+                router_logits[audio_mask] = audio_router_logits
+        else:
+            # router_logits: (num_tokens, n_experts)
+            router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.experts(
+            hidden_states=hidden_states, router_logits=router_logits
+        )
+
+        if self.num_shared_experts > 0:
+            final_hidden_states = final_hidden_states + shared_output
+
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+        return final_hidden_states.view(num_tokens, hidden_size)
+
+
+
+class BailingMoeBlock(nn.Module):
+
+    def __init__(
+            self,
+            config: BailingMoeConfig,
+            cache_config: Optional[CacheConfig] = None,
+            quant_config: Optional[QuantizationConfig] = None,
+            prefix: str = "",
+    ):
+        super().__init__()
+        self.config = config
+        hidden_size = config.hidden_size
+        intermediate_size = config.intermediate_size
+        self.input_layernorm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
+        self.attention = BailingAttention(config,
+                                      cache_config,
+                                      quant_config,
+                                      prefix=f"{prefix}.attention")
+        self.post_attention_layernorm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
+        if getattr(config, 'multi_gate', False):
+            self.mlp = BailingSparseMoe(intermediate_size, config, quant_config, True, prefix=f"{prefix}.mlp")
+        else:
+            self.mlp = BailingMoE(intermediate_size, config, quant_config, True, prefix=f"{prefix}.mlp")
+
+    def forward(
+            self,
+            hidden_states: torch.Tensor,
+            position_ids: torch.Tensor,
+            residual: Optional[torch.Tensor],
+            intermediate_tensors: Optional[IntermediateTensors],
+    ) -> torch.Tensor:
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+
+        hidden_states = self.attention(
+            hidden_states=hidden_states,
+            position_ids=position_ids,
+        )
+
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        if isinstance(self.mlp, BailingSparseMoe):
+            hidden_states = self.mlp(hidden_states, intermediate_tensors)
+        else:
+            hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+class BailingMoeModel(nn.Module):
+
+    def __init__(
+            self,
+            *, 
+            vllm_config: VllmConfig,
+            prefix: str = "",
+    ):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.config = config
+        self.vocab_size = config.vocab_size
+        self.embed_dim = config.hidden_size
+
+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
+                                            and get_pp_group().is_last_rank):
+            self.word_embeddings = VocabParallelEmbedding(self.vocab_size, self.embed_dim)
+        else:
+            self.word_embeddings = PPMissingLayer()
+
+        self.embedding_dropout = torch.nn.Dropout(config.embedding_dropout)
+
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: BailingMoeBlock(
+                config=config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+                prefix=prefix,
+            ),
+            prefix=f"{prefix}.layers"
+        )
+
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size
+            )
+        )
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(self.embed_dim, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+    
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.word_embeddings(input_ids)
+
+    def forward(
+            self,
+            input_ids: torch.Tensor,
+            position_ids: torch.Tensor,
+            intermediate_tensors: Optional[IntermediateTensors],
+            inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            hidden_states, residual = layer(
+                hidden_states,
+                position_ids,
+                residual,
+                intermediate_tensors,
+            )
+
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+
+class BailingMoeForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
+
+    packed_modules_mapping = {
+        "query_key_value": ["query_key_value"],
+        "dense_h_to_4h": ["dense_h_to_4h"],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    # LoRA specific attributes
+    supported_lora_modules = [
+        "query_key_value",
+        "dense",
+        "dense_h_to_4h",
+        "dense_4h_to_h",
+        "gate_up_proj",
+        "down_proj",
+    ]
+    embedding_modules = {}
+    embedding_padding_modules = []
+
+    def __init__(
+            self,
+            *,
+            vllm_config: VllmConfig,
+            prefix: str = "",
+    ) -> None:
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        if hasattr(config, "llm_config"):
+            config = config.llm_config
+            vllm_config.model_config.hf_config = config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+        self.quant_config = quant_config
+        self.max_position_embeddings = config.max_position_embeddings
+        self.model = BailingMoeModel(
+                                    vllm_config=vllm_config,
+                                    prefix=maybe_prefix(prefix, "model")
+        )
+        if get_pp_group().is_last_rank:
+            self.lm_head = self.word_embeddings if config.tie_word_embeddings \
+                else ParallelLMHead(config.vocab_size, config.hidden_size, quant_config=quant_config)
+            self.logits_processor = LogitsProcessor(config.vocab_size)
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.sampler = get_sampler()
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors
+        )
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+            self,
+            input_ids: torch.Tensor,
+            positions: torch.Tensor,
+            intermediate_tensors: Optional[IntermediateTensors] = None,
+            inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        model_output = self.model(input_ids, positions, intermediate_tensors,
+                                         inputs_embeds)
+        return model_output
+
+    def compute_logits(
+            self,
+            hidden_states: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states, sampling_metadata)
+        return logits
+
+    def sample(
+            self,
+            logits: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.num_experts)
+
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            # lm_head 在tie_word_embeddings=true时跳过，与embeddings共享；反之则独立加载
+            if (("v_head" in name) or ("inv_freq" in name) or
+                    (self.config.tie_word_embeddings and "lm_head" in name)):
+                continue
+
+            # 如果是norm head的方式，需要在初始化的时候对lm_head进行处理
+            if self.config.norm_head and "lm_head.weight" in name:
+                import torch.nn.functional as F
+                loaded_weight = F.normalize(loaded_weight, dim=0, p=2, eps=1e-7)
+
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                if "mlp.experts" in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                # 跳过不存在的module key
+                if name not in params_dict:
+                    continue
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    # Skip loading extra bias for GPTQ models.
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+                    # 跳过不存在的module key
+                    if name not in params_dict:
+                        continue
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader", default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
diff --git a/vllm/model_executor/models/bailingmm.py b/vllm/model_executor/models/bailingmm.py
new file mode 100644
index 000000000..9ce664ffb
--- /dev/null
+++ b/vllm/model_executor/models/bailingmm.py
@@ -0,0 +1,980 @@
+from vllm import ModelRegistry
+from vllm.model_executor import SamplingMetadata
+from vllm.model_executor.models.qwen3_moe import Qwen3MoeForCausalLM
+
+from vllm.attention import AttentionMetadata
+from vllm.sequence import IntermediateTensors
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
+from vllm.model_executor.layers.sampler import SamplerOutput
+from vllm.model_executor.models.interfaces import SupportsMultiModal, SupportsPP
+from vllm.model_executor.models.qwen2_audio import (
+            Qwen2AudioInputs, Qwen2AudioProcessingInfo,
+                _get_feat_extract_output_lengths)
+from vllm.model_executor.models.qwen2_5_vl import (
+    Qwen2_5_VisionTransformer, Qwen2_5_VLImageEmbeddingInputs,
+    Qwen2_5_VLImageInputs, Qwen2_5_VLImagePixelInputs,
+    Qwen2_5_VLProcessingInfo, Qwen2_5_VLVideoEmbeddingInputs,
+    Qwen2_5_VLVideoInputs, Qwen2_5_VLVideoPixelInputs)
+from vllm.multimodal.inputs import MultiModalInputs
+from vllm.multimodal import  MultiModalDataDict
+from vllm.multimodal.processing import (BaseMultiModalProcessor,
+                                        BaseProcessingInfo, PromptReplacement,
+                                        PromptUpdate)
+from vllm.model_executor.models.qwen2_vl import Qwen2VLMultiModalDataParser
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.multimodal.inputs import (ImageItem, ModalityData,
+                                    MultiModalFieldConfig, MultiModalKwargs,
+                                    NestedTensors)
+from vllm.multimodal.parse import (AudioProcessorItems, DictEmbeddingItems,
+                                   ModalityDataItems, MultiModalDataItems,
+                                   MultiModalDataParser, ImageSize, AudioItem)
+from vllm.multimodal.processing import (BaseMultiModalProcessor,
+                                        PromptReplacement, PromptUpdate)
+from vllm.multimodal.profiling import BaseDummyInputsBuilder, ProcessorInputs
+from vllm.sequence import IntermediateTensors, SequenceData
+from vllm.transformers_utils.processor import (
+    cached_feature_extractor_from_config, cached_image_processor_from_config)
+
+from vllm.inputs import INPUT_REGISTRY, InputContext, token_inputs
+from vllm.distributed import get_tensor_model_parallel_world_size
+from vllm.utils import LRUCache
+
+from functools import cached_property, partial, lru_cache
+from typing import (Any, Dict, Iterable, List, Mapping, Optional, Sequence,
+                    Set, Tuple, Union)
+import torch
+from torch import nn
+import torch.nn.functional as F
+import copy
+import json
+import sys
+import os
+
+from transformers import (
+    AutoProcessor,
+    AutoTokenizer,
+    GenerationConfig
+)
+from transformers.feature_extraction_utils import BatchFeature
+from transformers.models.qwen2_5_omni.configuration_qwen2_5_omni import (
+    Qwen2_5OmniConfig, Qwen2_5OmniThinkerConfig)
+from transformers.models.qwen2_5_omni.modeling_qwen2_5_omni import (
+    Qwen2_5OmniAudioEncoder)
+from transformers.models.qwen2_5_omni.processing_qwen2_5_omni import (
+    Qwen2_5OmniProcessor)
+
+from .utils import AutoWeightsLoader, init_vllm_registered_model, maybe_prefix
+
+def ming_lite_field_config(hf_inputs: Mapping[str, torch.Tensor]):
+    audio_feats_lengths = hf_inputs.get("audio_feats_lengths", torch.empty((0, )))
+    if len(audio_feats_lengths.shape) > 1:
+        audio_feats_lengths = audio_feats_lengths.squeeze(0)
+    image_grid_thw = hf_inputs.get("image_grid_thw", torch.empty((0, 3)))
+    image_grid_sizes = image_grid_thw.prod(-1)
+
+    video_grid_thw = hf_inputs.get("video_grid_thw", torch.empty((0, 3)))
+    video_grid_sizes = video_grid_thw.prod(-1)
+
+    return dict(
+        audio_feats=MultiModalFieldConfig.flat_from_sizes(
+            "audio", audio_feats_lengths, dim=1),
+        audio_placeholder_loc_lens=MultiModalFieldConfig.flat_from_sizes(
+            "audio", audio_feats_lengths, dim=1),
+        audio_feats_lengths=MultiModalFieldConfig.batched("audio"),
+        pixel_values=MultiModalFieldConfig.flat_from_sizes(
+            "image", image_grid_sizes),
+        image_grid_thw=MultiModalFieldConfig.batched("image"),
+        pixel_values_videos=MultiModalFieldConfig.flat_from_sizes(
+            "video", video_grid_sizes),
+        video_grid_thw=MultiModalFieldConfig.batched("video"),
+    )
+
+class BailingMMProcessingInfo(BaseProcessingInfo):
+
+    def get_hf_config(self):
+        return self.ctx.get_hf_config(BailingMMConfig)
+
+    def get_hf_processor(
+        self, **kwargs: object,
+    ):
+        from processing_bailingmm import BailingMMProcessor
+
+        processor = self.ctx.get_hf_processor(BailingMMProcessor, **kwargs)
+        return processor
+
+    def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:
+        return {"image": None, "video": None, "audio" :None}
+
+    def get_tokenizer(self):
+        tokenizer = AutoTokenizer.from_pretrained(self.ctx.model_config.model, trust_remote_code=True) 
+        return tokenizer
+
+    def get_feature_extractor(self):
+        hf_processor = self.get_hf_processor()
+        audio_processor = hf_processor.audio_processor
+        feature_extractor = audio_processor
+        return feature_extractor
+
+    def get_image_size_with_most_features(self):
+        image_processor = self.get_hf_processor().image_processor
+        patch_size = image_processor.patch_size
+        merge_size = image_processor.merge_size
+        if image_processor.do_resize:
+            from image_processing_bailingmm import smart_resize
+            resized_height, resized_width = smart_resize(
+                height=999,
+                width=999,
+                factor=patch_size * merge_size,
+                min_pixels=image_processor.min_pixels,
+                max_pixels=image_processor.max_pixels,
+            )
+            preprocessed_size = ImageSize(width=resized_width, height=resized_height)
+        else:
+            preprocessed_size = ImageSize(width=999, height=999)
+        return preprocessed_size
+
+    def get_num_frames_with_most_features(
+        self,
+        seq_len: int,
+        mm_counts: Mapping[str, int],
+        ) -> int:
+        # TODO(serina.wzq): fill this function
+        return 32
+
+
+class BailingMultiModalDataParser(MultiModalDataParser):
+    def _parse_audio_data(
+        self,
+        data: Union[dict[str, torch.Tensor], ModalityData[AudioItem]],
+    ) -> Optional[ModalityDataItems[Any, Any]]:
+        if isinstance(data[0], tuple):
+            audio_data = {"audio":data[0][0], "sr":data[0][1]}
+            return AudioProcessorItems(data[0][0].numpy())
+        return super()._parse_audio_data(data)
+
+
+class BailingMultiModalProcessor(
+        BaseMultiModalProcessor[BailingMMProcessingInfo]):
+    def _get_data_parser(self) -> MultiModalDataParser:
+        feature_extractor = self.info.get_feature_extractor()
+        return BailingMultiModalDataParser(
+                target_sr=feature_extractor.sample_rate)
+
+    def _call_hf_processor(
+        self,
+        prompt: str,
+        mm_data: Mapping[str, object],
+        mm_kwargs: Mapping[str, object],
+    ) -> BatchFeature:
+        processor = self.info.get_hf_processor()
+
+        image_inputs = mm_data.get("images", None)
+        video_inputs = mm_data.get("videos", None)
+        audio_inputs = mm_data.get("audios", None)
+        if audio_inputs is not None:
+            audio = mm_data["audios"][0]
+            audio_inputs = (torch.tensor(audio), processor.audio_processor.sample_rate)
+            batch_inputs = processor(text=[prompt], images=image_inputs, videos=video_inputs, audios=[audio_inputs], return_tensors="pt", audio_kwargs={'use_whisper_encoder': True})
+        else:
+            batch_inputs = processor(text=[prompt], images=image_inputs, videos=video_inputs, return_tensors="pt", audio_kwargs={'use_whisper_encoder': True})
+        input_ids = batch_inputs['input_ids']
+        mm_inputs = {}
+
+        for k in batch_inputs.keys():
+            if k == "pixel_values" or k == "pixel_values_videos" or k == "audio_feats":
+                batch_inputs[k] = batch_inputs[k].to(dtype=torch.bfloat16)
+            if k != "input_ids":
+                mm_inputs[k] = batch_inputs[k]
+
+        if os.getenv("IMAGE_GEN_MODE") == "EDIT":
+            assert input_ids.size(0) == 1
+            input_ids = torch.cat([
+                input_ids, 
+                torch.zeros((1, 342), dtype=input_ids.dtype, device=input_ids.device),
+            ], dim=1)
+
+        elif os.getenv("IMAGE_GEN_MODE") == "T2I":
+            assert input_ids.size(0) == 1
+            input_ids_list = input_ids[0].tolist()
+            found_token_198 = False
+            for id_idx in range(len(input_ids_list)):
+                if input_ids_list[id_idx] == 198:
+                    if id_idx > 0 and input_ids_list[id_idx-1] == 126348:
+                        input_ids_list[id_idx] = 126348
+                        found_token_198 = True
+                        break
+            
+            assert found_token_198 is True
+
+            input_ids = torch.tensor([[i for i in input_ids_list if i not in [126346, 126347, 126348]]])
+
+            input_ids = torch.cat([
+                input_ids, 
+                torch.tensor([[126347,126346,126348]], dtype=input_ids.dtype, device=input_ids.device),
+            ], dim=1)
+            input_ids = torch.cat([
+                input_ids, 
+                torch.zeros((1, 342-3), dtype=input_ids.dtype, device=input_ids.device),
+            ], dim=1)
+            
+            mm_inputs["image_grid_thw"] = torch.tensor([[ 1, 2, 2]])
+            mm_inputs["pixel_values"] = mm_inputs["pixel_values"][:4,:]
+
+            
+
+        return BatchFeature({
+            "input_ids": input_ids,
+            **mm_inputs,
+        })
+
+    def _get_mm_fields_config(
+        self,
+        hf_inputs: BatchFeature,
+        hf_processor_mm_kwargs: Mapping[str, object],
+    ) -> Mapping[str, MultiModalFieldConfig]:
+        return ming_lite_field_config(hf_inputs)
+
+    def _get_prompt_updates(
+        self,
+        mm_items: MultiModalDataItems,
+        hf_processor_mm_kwargs: Mapping[str, Any],
+        out_mm_kwargs: MultiModalKwargs,
+    ) -> Sequence[PromptUpdate]:
+        processor = self.info.get_hf_processor()
+        tokenizer = self.info.get_tokenizer()
+        image_processor = processor.image_processor
+        vocab = tokenizer.get_vocab()
+
+        audio_token = processor.audio_token
+        image_token = processor.image_token
+        video_token = processor.video_token
+
+        audio_token_id = vocab["<audioPatch>"]
+        image_token_id = vocab["<imagePatch>"]
+        video_token_id = vocab["<imagePatch>"]
+
+        audio_feats_lengths = out_mm_kwargs.get("audio_feats_lengths")
+        def get_replacement_bailingmm_audio(item_idx: int):
+            num_audio_feats = audio_feats_lengths[item_idx]
+            num_whisper_encoder_features = ((num_audio_feats-3+2*1)//2+1-3+2*1)//2+1
+            if num_whisper_encoder_features == 0:
+                audios = mm_items.get_items("audio", AudioProcessorItems)
+                audio = audios.get(item_idx)
+                raise ValueError(
+                    f"The audio {audio} (len={len(audio)}) is too short "
+                    "to be represented inside the model")
+
+            return [audio_token_id] * num_whisper_encoder_features
+
+        def get_replacement_bailingmm_vision(item_idx: int, modality: str):
+            grid_thw = out_mm_kwargs[f"{modality}_grid_thw"][item_idx]
+            assert isinstance(grid_thw, torch.Tensor)
+            merge_length = image_processor.merge_size**2
+
+            token_id = image_token_id if modality == "image" else video_token_id
+            return [token_id] * (int(grid_thw.prod()) // merge_length)
+
+        return [
+            PromptReplacement(
+                modality="audio",
+                target=audio_token_id,
+                replacement=get_replacement_bailingmm_audio,
+            ),
+            PromptReplacement(
+                modality="image",
+                target=image_token_id,
+                replacement=partial(get_replacement_bailingmm_vision,
+                                    modality="image"),
+            ),
+            PromptReplacement(
+                modality="video",
+                target=video_token_id,
+                replacement=partial(get_replacement_bailingmm_vision,
+                                    modality="video"),
+            ),
+        ]
+
+    def _cached_apply_hf_processor(
+        self,
+        prompt: Union[str, list[int]],
+        mm_data_items: MultiModalDataItems,
+        hf_processor_mm_kwargs: Mapping[str, object],
+    ) -> tuple[list[int], MultiModalKwargs, bool]:
+
+        prompt_ids, mm_kwargs, _ = self._apply_hf_processor_main(
+            prompt=prompt,
+            mm_items=mm_data_items,
+            hf_processor_mm_kwargs=hf_processor_mm_kwargs,
+            enable_hf_prompt_update=True,
+        )
+        # NOTE: The tokens are already inserted by the BailingMMProcessor
+        return prompt_ids, mm_kwargs, True
+
+
+
+class BailingMMDummyInputsBuilder(
+        BaseDummyInputsBuilder[BailingMMProcessingInfo]):
+    _processor_inputs_cache: LRUCache = LRUCache(capacity=1024)
+    def get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:
+        num_images = 1
+        num_videos = mm_counts.get("video", 0)
+        num_audios = mm_counts.get("audio", 0)
+
+        hf_processor = self.info.get_hf_processor()
+        image_token: str = hf_processor.image_token
+        video_token: str = hf_processor.video_token
+        audio_token: str = hf_processor.audio_token
+        image_token = "<IMAGE>"
+        video_token = "<VIDEO>"
+        audio_token = "<AUDIO>"
+
+        return image_token * num_images + video_token * num_videos + audio_token * num_audios
+
+    def get_dummy_mm_data(
+        self,
+        seq_len: int,
+        mm_counts: Mapping[str, int],
+    ) -> MultiModalDataDict:
+        num_images = 1
+        num_videos = mm_counts.get("video", 0)
+        num_audios = mm_counts.get("audio", 0)
+        feature_extractor = self.info.get_feature_extractor()
+        target_audio_length = 25 * feature_extractor.sample_rate
+        target_width, target_height = \
+            self.info.get_image_size_with_most_features()
+        target_num_frames = \
+            self.info.get_num_frames_with_most_features(seq_len, mm_counts)
+
+        return {
+            "audio":
+            self._get_dummy_audios(length=target_audio_length,
+                                   num_audios=num_audios),
+            "image":
+            self._get_dummy_images(width=target_width,
+                                   height=target_height,
+                                   num_images=num_images),
+            "video":
+            self._get_dummy_videos(
+                width=target_width,
+                height=target_height,
+                num_frames=target_num_frames,
+                num_videos=num_videos,
+            )
+        }
+@MULTIMODAL_REGISTRY.register_processor(
+    BailingMultiModalProcessor,
+    info=BailingMMProcessingInfo,
+    dummy_inputs=BailingMMDummyInputsBuilder,
+)
+class BailingMMNativeForConditionalGeneration(nn.Module, SupportsMultiModal, SupportsPP):
+    def __init__(self,
+                 *,
+                 vllm_config: VllmConfig,
+                 model_path: Optional[str] = None,
+                 prefix: str="") -> None:
+        super().__init__()
+        self.model_path = model_path
+        ## for multimodal encoders
+        from modeling_utils import Transpose, encode_audio_segments, patch_continuous_features, build_modality_mask
+        # audio encoder
+        from funasr.models.sanm.encoder import SANMEncoder
+        #vision encoder
+        from qwen2_5_vit import Qwen2_5_VisionTransformer
+        # whisper encoder
+        from modeling_whisper_encoder import WhisperAudioEncoder
+        # talker
+        from modeling_bailing_talker import BailingTalkerForConditionalGeneration
+        # init llm
+        self.config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.model = init_vllm_registered_model(
+            vllm_config=vllm_config,
+            hf_config=self.config.llm_config,
+            prefix=maybe_prefix(prefix, "model"),
+            architectures=["BailingMoeForCausalLM"],)
+        self.word_embeddings = self.model.model.word_embeddings
+
+        # init encoder
+        if self.config.vision_config:
+            self.vision = Qwen2_5_VisionTransformer(self.config.vision_config)
+        if self.config.audio_config:
+            from safetensors.torch import load_file, save_file
+            import os
+            if hasattr(self.config, "quantization_config"):
+                positional_embedding = load_file(os.path.join(self.config.name_or_path, "model-00001-of-00005.safetensors"))["audio.positional_embedding"]
+            else:
+                positional_embedding = load_file(os.path.join(self.config.name_or_path, "model-00001-of-00008.safetensors"))["audio.positional_embedding"]
+            self.audio = WhisperAudioEncoder(**self.config.audio_config.whisper_encoder_config)
+            self.audio.positional_embedding = positional_embedding
+
+
+        mlp_modules_img = [nn.Linear(self.vision.image_emb_dim, self.model.config.hidden_size)]
+        for _ in range(1, self.config.mlp_depth):
+            mlp_modules_img.append(nn.GELU())
+            mlp_modules_img.append(nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size))
+        self.linear_proj = nn.Sequential(*mlp_modules_img)
+        self.img_gen = True 
+
+        if self.audio:
+            audio_encoder_proj = torch.nn.Conv1d(
+                self.audio.audio_emb_dim,
+                self.model.config.hidden_size,
+                kernel_size=self.config.audio_config.ds_kernel_size,
+                stride=self.config.audio_config.ds_stride,
+                padding=self.config.audio_config.ds_kernel_size // 2,
+            )
+            mlp_modules_audio = [audio_encoder_proj, Transpose(-1, -2)]
+            for _ in range(1, self.config.mlp_depth):
+                mlp_modules_audio.append(nn.GELU())
+                mlp_modules_audio.append(nn.Linear(
+                    self.model.config.hidden_size, self.model.config.hidden_size
+                ))
+            mlp_modules_audio.append(Transpose(-1, -2)) # Revert to a conv-style permutation.
+            self.linear_proj_audio = nn.Sequential(*mlp_modules_audio)
+        if self.img_gen:
+            temp_state_dict = load_file(os.path.join(self.config.name_or_path, 'mlp', 'model.safetensors'))
+            self.query_tokens_dict = nn.ParameterDict()
+            self.img_gen_scales = [4, 8, 16]
+            for scale in self.img_gen_scales:                    
+                num_tokens = scale * scale
+                scale_name = f"{scale}x{scale}"
+                #weights = temp_state_dict[f"query_tokens_dict.{scale_name}"]
+                self.query_tokens_dict[scale_name] = nn.Parameter(
+                    torch.nn.functional.normalize(torch.randn(num_tokens, self.model.config.hidden_size), dim=-1)
+                )
+            self.query_tokens_dict.to(torch.bfloat16).to('cuda')
+            modified_state_dict_query_tokens = {
+                f"{scale}x{scale}": temp_state_dict[f"query_tokens_dict.{scale}x{scale}"]
+                for scale in self.img_gen_scales   
+            }
+            self.query_tokens_dict.load_state_dict(modified_state_dict_query_tokens, strict=True)
+
+
+    def _parse_and_validate_image_input(self, **kwargs: object) -> Optional[torch.Tensor]:
+        pixel_values = kwargs.pop("pixel_values", None)
+        image_embeds = kwargs.pop("image_embeds", None)
+        image_grid_thw = kwargs.pop("image_grid_thw", None)
+        pixel_values_videos = kwargs.pop("pixel_values_videos", None)
+        video_grid_thw = kwargs.pop("video_grid_thw", None)
+        audio_feats = kwargs.pop("audio_feats", None)
+        audio_feats_lengths = kwargs.pop("audio_feats_lengths", None)
+        audio_placeholder_loc_lens = kwargs.pop("audio_placeholder_loc_lens", None)
+        return pixel_values, image_embeds, image_grid_thw, pixel_values_videos, video_grid_thw, audio_feats, audio_feats_lengths, audio_placeholder_loc_lens
+
+    def cal_vision_encoder(self, pixel_values, grid_thw):
+        if len(grid_thw.shape) > 2:
+            if len(pixel_values.shape) <= 2:
+                grid_thw = grid_thw.squeeze(0)
+            else:
+                grid_thw = grid_thw.squeeze(-2)
+        image_embeds = self.vision(pixel_values, grid_thw=grid_thw)
+        return image_embeds
+
+
+    def extract_image_feature(self, pixel_values, grid_thw):
+        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
+            image_embeds = None
+            if isinstance(pixel_values, list):
+                idx = 0
+                for img in pixel_values:
+                    image_embed = self.cal_vision_encoder(img, grid_thw=grid_thw[idx].unsqueeze(0))
+                    if image_embeds is None:
+                        image_embeds = image_embed
+                    else:
+                        image_embeds = torch.cat([image_embeds, image_embed], dim=0)
+                    idx = idx + 1
+            else:
+                image_embeds = self.cal_vision_encoder(pixel_values, grid_thw)
+            image_embeds = image_embeds.float()
+            image_embeds = self.linear_proj(image_embeds)
+        image_embeds = F.normalize(image_embeds, dim=-1)
+        return image_embeds 
+
+    def extract_audio_feature(self, audio_feats, audio_feats_lengths):
+        encoder = self.audio
+        proj_layer = self.linear_proj_audio
+        #[b, 1, T, n_mels] -> [b, T, n_mels]
+        audio_feats = audio_feats.squeeze(-3)
+        #[b, 1, len] -> [b, len]
+        audio_feats_lengths = audio_feats_lengths.squeeze(-2)
+        from modeling_utils import encode_audio_segments
+        audio_embeds, _, audio_embeds_lengths = encode_audio_segments(
+            encoder=encoder,
+            proj_layer=proj_layer,
+            wav_feats=audio_feats,
+            wav_feats_lengths=audio_feats_lengths,
+            audio_config=self.config.audio_config,
+        )
+        if self.config.audio_config.norm_query_embeds:
+            audio_embeds = F.normalize(audio_embeds, dim=2) # [-1, 256, 2048]
+        return audio_embeds.to(audio_feats.dtype), audio_embeds_lengths
+
+    def prompt_wrap_vision(self, input_ids, inputs_embeds, vision_embeds, image_token_id=None):
+        if vision_embeds is None or input_ids is None:
+            return inputs_embeds
+
+        if len(vision_embeds.shape) == 3:
+            vision_embeds = vision_embeds.reshape(-1, vision_embeds.shape[-1])
+
+        self.config.llm_config.image_patch_token = image_token_id if image_token_id is not None else self.config.llm_config.image_patch_token
+        n_image_tokens = (input_ids == self.config.llm_config.image_patch_token).sum().item()
+        n_image_features = vision_embeds.shape[0]
+        if n_image_tokens != n_image_features:
+            raise ValueError(
+                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
+            )
+        image_router_mask = (
+            (input_ids == self.config.llm_config.image_patch_token)
+            .to(inputs_embeds.device)
+        )
+        image_embeds = vision_embeds.to(inputs_embeds.device, inputs_embeds.dtype)
+        inputs_embeds[image_router_mask] = image_embeds
+        return inputs_embeds, image_router_mask
+
+    def prompt_wrap_image_and_video(self, input_ids, inputs_embeds, query_embeds_image, query_embeds_video, image_token_id=None):
+        self.config.llm_config.image_patch_token = image_token_id if image_token_id is not None else self.config.llm_config.image_patch_token
+        IMAGE_ST_TOKEN_ID = self.config.llm_config.image_start_token
+        IMAGE_ED_TOKEN_ID = self.config.llm_config.image_end_token
+        VIDEO_ST_TOKEN_ID = self.config.llm_config.video_start_token
+        VIDEO_ED_TOEKN_ID = self.config.llm_config.video_end_token
+        image_start_ids = torch.where(input_ids == IMAGE_ST_TOKEN_ID)[0]
+        image_end_ids = torch.where(input_ids == IMAGE_ED_TOKEN_ID)[0]
+        video_start_ids = torch.where(input_ids == VIDEO_ST_TOKEN_ID)[0]
+        video_end_ids = torch.where(input_ids == VIDEO_ED_TOEKN_ID)[0]
+        assert len(image_start_ids) == len(image_end_ids)
+        assert len(video_start_ids) == len(video_end_ids)
+        n_image_tokens = (input_ids == self.config.llm_config.image_patch_token).sum().item()
+        n_image_features = query_embeds_image.shape[0] + query_embeds_video.shape[0]
+        if n_image_tokens != n_image_features:
+            raise ValueError(
+                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
+            )
+        image_router_mask = (
+            (input_ids == self.config.llm_config.image_patch_token)
+            .to(inputs_embeds.device)
+        )
+        image_embeds = query_embeds_image.to(inputs_embeds.device, inputs_embeds.dtype)
+        video_embeds = query_embeds_video.to(inputs_embeds.device, inputs_embeds.dtype)
+        # merge image embeddings
+        cursor = 0
+        for i in range(len(image_start_ids)):
+            st = image_start_ids[i] + 1
+            ed = image_end_ids[i] - 1
+            num_features = image_end_ids[i] - image_start_ids[i] - 1
+            inputs_embeds[st:ed, :] = image_embeds[cursor:cursor+num_features-1, :]
+            cursor = cursor+num_features
+        cursor = 0
+        for i in range(len(video_start_ids)):
+            st = video_start_ids[i] + 1
+            ed = video_end_ids[i] - 1
+            num_features = video_end_ids[i] - video_start_ids[i] - 1
+            inputs_embeds[st:ed, :] = video_embeds[cursor:cursor+num_features-1, :]
+            cursor = cursor+num_features
+        # merge video embeddings
+        return inputs_embeds, image_router_mask
+
+    def prompt_wrap_audio(self, input_ids, inputs_embeds, audio_embeds, audio_embeds_lengths, placeholder_audio_loc_lens):
+        AUDIO_PAD_TOKEN_IDS =  self.config.llm_config.audio_patch_token
+        if len(audio_embeds.shape) == 3:
+            audio_embeds = audio_embeds.reshape(-1, audio_embeds.shape[-1])
+        n_audio_tokens = (input_ids == AUDIO_PAD_TOKEN_IDS).sum().item()
+        n_audio_features = audio_embeds.shape[0]
+        if n_audio_tokens != n_audio_features:
+            raise ValueError(
+                f"Audio features and audio tokens do not match: tokens: {n_audio_tokens}, eatures {n_audio_features}"
+            )
+        audio_router_mask = ((input_ids == AUDIO_PAD_TOKEN_IDS).to(inputs_embeds.device))
+        audio_embeds = audio_embeds.to(inputs_embeds.device, inputs_embeds.dtype)
+        inputs_embeds[audio_router_mask] = audio_embeds
+        return inputs_embeds, audio_router_mask
+
+    def prompt_wrap_navit(self, input_ids, query_embeds_image=None, query_embeds_video=None, query_embeds_audio=None,
+        query_embeds_audio_lengths=None, placeholder_audio_loc_lens=None, target_embeds=None):
+        inputs_embeds = self.word_embeddings(input_ids)
+        if query_embeds_image is None and query_embeds_video is None and query_embeds_audio is None and target_embeds is None:
+            return inputs_embeds
+        image_mask = None
+        audio_mask = None
+        if query_embeds_image is not None and query_embeds_video is None:
+            inputs_embeds, image_mask = self.prompt_wrap_vision(input_ids, inputs_embeds, query_embeds_image)
+        elif query_embeds_video is not None and query_embeds_image is None:
+            inputs_embeds, image_mask = self.prompt_wrap_vision(input_ids, inputs_embeds, query_embeds_video)
+        elif query_embeds_image is not None and query_embeds_video is not None:
+            inputs_embeds, image_mask = self.prompt_wrap_image_and_video(input_ids, inputs_embeds, query_embeds_image, query_embeds_video)
+        if query_embeds_audio is not None:
+            inputs_embeds, audio_mask = self.prompt_wrap_audio(
+                input_ids, inputs_embeds, query_embeds_audio, query_embeds_audio_lengths, placeholder_audio_loc_lens,
+            )
+        return inputs_embeds, image_mask, audio_mask
+
+    def append_input_ids_with_multiscale_learnable_tokens(
+        self,
+        text_ids,
+        attention_mask,
+        scales,
+        start_token_id,
+        end_token_id,
+        patch_token_id,
+    ):
+        
+        assert text_ids.ndim == 1
+        #max_alloc_len = 32768
+        raw_text_ids_len = text_ids.size(0)
+
+        first_padding_ind = raw_text_ids_len - text_ids.flip(0).gt(0).int().argmax().cpu().item()
+
+        if os.getenv("IMAGE_GEN_MODE") == "EDIT":
+            assert raw_text_ids_len - first_padding_ind == 342
+            text_ids = text_ids[:first_padding_ind]
+        else:
+            assert os.getenv("IMAGE_GEN_MODE") == "T2I"
+            assert raw_text_ids_len - first_padding_ind == 342 - 3
+            text_ids = text_ids[:first_padding_ind-3]
+
+        text_ids = text_ids.unsqueeze(0)
+        #assert attention_mask.shape == text_ids.shape
+        gen_mask = torch.zeros_like(text_ids)
+        for scale in scales:
+            text_ids = torch.cat(
+                [
+                    text_ids,
+                    torch.tensor([[start_token_id]]).to(text_ids.dtype).to(text_ids.device),
+                    torch.tensor([[patch_token_id] * (scale**2)])
+                    .to(text_ids.dtype)
+                    .to(text_ids.device),
+                    torch.tensor([[end_token_id]]).to(text_ids.dtype).to(text_ids.device),
+                ],
+                dim=1,
+            )
+            
+            #attention_mask = torch.cat(
+            #    [
+            #        attention_mask,
+            #        torch.tensor([[1] * ((scale**2) + 2)])
+            #        .to(attention_mask.dtype)
+            #        .to(attention_mask.device),
+            #    ],
+            #    dim=1,
+            #)
+            gen_mask = torch.cat(
+                [
+                    gen_mask,
+                    torch.tensor([[0]]).to(gen_mask.dtype).to(gen_mask.device),
+                    torch.tensor([[1] * (scale**2)]).to(gen_mask.dtype).to(gen_mask.device),
+                    torch.tensor([[0]]).to(gen_mask.dtype).to(gen_mask.device),
+                ],
+                dim=1,
+            )
+
+        if text_ids.size(1) < raw_text_ids_len:
+            assert text_ids.size(1) < raw_text_ids_len, (text_ids.size(1), raw_text_ids_len)
+            assert text_ids.shape == gen_mask.shape
+
+            text_ids = torch.cat([
+                text_ids, 
+                torch.zeros((1, raw_text_ids_len - text_ids.size(1)), dtype=text_ids.dtype, device=text_ids.device),
+            ], dim=1)
+            gen_mask = torch.cat([
+                gen_mask, 
+                torch.zeros((1, raw_text_ids_len - gen_mask.size(1)), dtype=gen_mask.dtype, device=gen_mask.device),
+            ], dim=1)
+
+        assert text_ids.shape == gen_mask.shape
+        return text_ids, None, gen_mask
+
+    def get_rope_index(
+        self,
+        input_ids,
+        image_token_id,
+        video_token_id,
+        image_start_token_id,
+        video_start_token_id,
+        image_grid_thw,
+        video_grid_thw,
+        attention_mask,
+        spatial_merge_size=2,
+        tokens_per_second=2,
+        second_per_grid_ts=None,
+    ):
+        use_abs_time_pos = second_per_grid_ts is not None
+
+        mrope_position_deltas = []
+        if image_grid_thw is not None or video_grid_thw is not None:
+            total_input_ids = input_ids
+            if attention_mask is None:
+                attention_mask = torch.ones_like(total_input_ids)
+            position_ids = torch.ones(
+                3,
+                input_ids.shape[0],
+                input_ids.shape[1],
+                dtype=input_ids.dtype,
+                device=input_ids.device,
+            )
+            image_index, video_index = 0, 0
+            attention_mask = attention_mask.to(total_input_ids.device)
+            for i, input_ids in enumerate(total_input_ids):
+                input_ids = input_ids[attention_mask[i] == 1]
+                image_nums, video_nums = 0, 0
+                if image_grid_thw is not None:
+                    vision_start_indices = torch.argwhere(
+                        input_ids == image_start_token_id
+                    ).squeeze(1)
+                    vision_tokens = input_ids[vision_start_indices + 1]
+                    image_nums = (vision_tokens == image_token_id).sum()
+                if video_grid_thw is not None:
+                    vision_start_indices = torch.argwhere(
+                        input_ids == video_start_token_id
+                    ).squeeze(1)
+                    vision_tokens = input_ids[vision_start_indices + 1]
+                    video_nums = (vision_tokens == video_token_id).sum()
+
+                input_tokens = input_ids.tolist()
+                llm_pos_ids_list: list = []
+                st = 0
+                remain_images, remain_videos = image_nums, video_nums
+                for _ in range(image_nums + video_nums):
+                    if image_token_id in input_tokens and remain_images > 0:
+                        ed_image = input_tokens.index(image_token_id, st)
+                    else:
+                        ed_image = len(input_tokens) + 1
+                    if video_token_id in input_tokens and remain_videos > 0:
+                        ed_video = input_tokens.index(video_token_id, st)
+                    else:
+                        ed_video = len(input_tokens) + 1
+                    if ed_image < ed_video:
+                        t, h, w = (
+                            image_grid_thw[image_index][0],
+                            image_grid_thw[image_index][1],
+                            image_grid_thw[image_index][2],
+                        )
+                        second_per_grid_t = 0
+                        image_index += 1
+                        remain_images -= 1
+                        ed = ed_image
+
+                    else:
+                        t, h, w = (
+                            video_grid_thw[video_index][0],
+                            video_grid_thw[video_index][1],
+                            video_grid_thw[video_index][2],
+                        )
+                        if second_per_grid_ts is not None:
+                            second_per_grid_t = second_per_grid_ts[video_index]
+                        else:
+                            second_per_grid_t = 1.0
+                        video_index += 1
+                        remain_videos -= 1
+                        ed = ed_video
+                    llm_grid_t, llm_grid_h, llm_grid_w = (
+                        t.item(),
+                        h.item() // spatial_merge_size,
+                        w.item() // spatial_merge_size,
+                    )
+                    text_len = ed - st
+
+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+                    llm_pos_ids_list.append(
+                        torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
+                    )
+
+                    range_tensor = torch.arange(llm_grid_t).view(-1, 1)
+                    expanded_range = range_tensor.expand(-1, llm_grid_h * llm_grid_w)
+                    if use_abs_time_pos:
+                        time_tensor = expanded_range * second_per_grid_t * tokens_per_second
+                        time_tensor_long = time_tensor.long()
+                    else:
+                        time_tensor_long = expanded_range.long()
+                    t_index = time_tensor_long.flatten()
+
+                    h_index = (
+                        torch.arange(llm_grid_h)
+                        .view(1, -1, 1)
+                        .expand(llm_grid_t, -1, llm_grid_w)
+                        .flatten()
+                    )
+                    w_index = (
+                        torch.arange(llm_grid_w)
+                        .view(1, 1, -1)
+                        .expand(llm_grid_t, llm_grid_h, -1)
+                        .flatten()
+                    )
+                    llm_pos_ids_list.append(
+                        torch.stack([t_index, h_index, w_index]) + text_len + st_idx
+                    )
+                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w
+
+                if st < len(input_tokens):
+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+                    text_len = len(input_tokens) - st
+                    llm_pos_ids_list.append(
+                        torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
+                    )
+
+                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
+                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
+                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
+            mrope_position_deltas = torch.tensor(
+                mrope_position_deltas, device=input_ids.device
+            ).unsqueeze(1)
+        else:
+            if attention_mask is not None:
+                position_ids = attention_mask.long().cumsum(-1) - 1
+                position_ids.masked_fill_(attention_mask == 0, 1)
+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)
+                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
+                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
+            else:
+                position_ids = (
+                    torch.arange(input_ids.shape[1], device=input_ids.device)
+                    .view(1, 1, -1)
+                    .expand(3, input_ids.shape[0], -1)
+                )
+                mrope_position_deltas = torch.zeros(
+                    [input_ids.shape[0], 1],
+                    device=input_ids.device,
+                    dtype=input_ids.dtype,
+                )
+
+        return position_ids, mrope_position_deltas
+
+    def forward(
+            self,
+            input_ids: torch.Tensor,
+            positions: torch.Tensor,
+            intermediate_tensors: Optional[IntermediateTensors] = None,
+            **kwargs
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        image_input, image_embeds, grid_thw, video_input, video_grid_thw, audio_input, audio_feats_lengths, audio_placeholder_loc_lens = self._parse_and_validate_image_input(**kwargs)
+        image_embeds, video_embeds, audio_embeddings, audio_embeds_lengths = None, None, None, None
+
+        tp_size = get_tensor_model_parallel_world_size()
+        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
+            if image_input is not None:
+                image_embeds = self.extract_image_feature(image_input, grid_thw=grid_thw)
+            if video_input is not None:
+                video_grid_thw = video_grid_thw.squeeze(-2)
+                video_embeds = self.extract_image_feature(video_input, grid_thw=video_grid_thw)
+            if audio_input is not None:
+                audio_embeddings, audio_embeds_lengths = self.extract_audio_feature(audio_input, audio_feats_lengths)
+                audio_placeholder_loc_lens = audio_placeholder_loc_lens.squeeze(1, 2)
+
+            position_ids = None
+            if self.img_gen and os.getenv("IMAGE_GEN_MODE") in ["EDIT", "T2I"]:
+                attention_mask = None
+                input_ids, attention_mask, gen_mask = self.append_input_ids_with_multiscale_learnable_tokens(
+                    input_ids,
+                    attention_mask,
+                    self.img_gen_scales,
+                    self.config.llm_config.image_patch_token + 1,
+                    self.config.llm_config.image_patch_token + 2,
+                    self.config.llm_config.image_patch_token,
+                )
+
+                query_tokens_embeds = torch.cat(
+                    [self.query_tokens_dict[f"{scale}x{scale}"] for scale in self.img_gen_scales], 
+                    dim=0,
+                )
+                if os.getenv("IMAGE_GEN_MODE") == "EDIT":
+                    assert image_embeds is not None
+                    image_embeds = torch.cat([image_embeds, query_tokens_embeds], dim=0)
+                else:
+                    assert os.getenv("IMAGE_GEN_MODE") == "T2I"
+                    image_embeds = query_tokens_embeds
+
+                new_image_grid_thw = []
+                for scale in self.img_gen_scales:
+                    new_image_grid_thw.append([1, 2, scale * scale * 2])
+
+                new_image_grid_thw = torch.tensor(new_image_grid_thw, dtype=input_ids.dtype).to(input_ids.device)
+
+                if os.getenv("IMAGE_GEN_MODE") == "EDIT":
+                    assert grid_thw is not None
+                    grid_thw = grid_thw.squeeze(0)
+                    grid_thw = torch.cat([grid_thw, new_image_grid_thw], dim=0)
+                else:
+                    assert os.getenv("IMAGE_GEN_MODE") == "T2I"
+                    grid_thw = new_image_grid_thw
+
+
+                VLLM_MAX_ALLOC_TOKEN = 32768
+                input_ids = input_ids[:,:VLLM_MAX_ALLOC_TOKEN]
+                position_ids, _ = self.get_rope_index(
+                    input_ids,
+                    image_token_id=self.config.llm_config.image_patch_token,
+                    video_token_id=self.config.llm_config.image_patch_token,
+                    image_start_token_id=self.config.llm_config.image_start_token,
+                    video_start_token_id=self.config.llm_config.video_start_token,
+                    image_grid_thw=grid_thw,
+                    video_grid_thw=None,
+                    attention_mask=attention_mask,
+                )
+                positions = position_ids.squeeze(1)
+                input_ids = input_ids.squeeze(0)
+            #TODO(wpf): position_ids maybe different with Hugging Face
+            #attention_mask = None
+            #grid_thw = grid_thw.squeeze(0)
+            #position_ids, _ = self.get_rope_index(
+            #    input_ids.unsqueeze(0),
+            #    image_token_id=self.config.llm_config.image_patch_token,
+            #    video_token_id=self.config.llm_config.image_patch_token,
+            #    image_start_token_id=self.config.llm_config.image_start_token,
+            #    video_start_token_id=self.config.llm_config.video_start_token,
+            #    image_grid_thw=grid_thw,
+            #    video_grid_thw=None,
+            #    attention_mask=attention_mask,
+            #)
+            #position_ids = position_ids.squeeze(1)
+            #assert position_ids.ndim == 2
+            #assert position_ids.size(0) == 3
+
+            if (image_embeds is None and video_embeds is None and audio_embeddings is None):
+                inputs_embeddings = self.word_embeddings(input_ids.clip(0, self.word_embeddings.weight.shape[0] * tp_size - 1))
+                if intermediate_tensors is not None:
+                    intermediate_tensors.__setitem__("inputs_embeds", inputs_embeddings)
+                else:
+                    intermediate_tensors = IntermediateTensors({"inputs_embeds": inputs_embeddings})
+            else:
+                #clip 此处做了检查防止超出词表大小
+                inputs_embeddings, image_mask, audio_mask = self.prompt_wrap_navit(
+                        input_ids.clip(0, self.word_embeddings.weight.shape[0] * tp_size - 1), image_embeds, video_embeds, audio_embeddings,
+                        audio_embeds_lengths, audio_placeholder_loc_lens, None,  # noqa
+                )
+                if intermediate_tensors is not None:
+                    intermediate_tensors.__setitem__("image_mask", image_mask)
+                    intermediate_tensors.__setitem__("audio_mask", audio_mask)
+                else:
+                    intermediate_tensors = IntermediateTensors({
+                        "image_mask": image_mask,
+                        "audio_mask": audio_mask,
+                    })
+
+        return self.model.forward(input_ids, positions, intermediate_tensors, inputs_embeddings)  
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
+        # audio encoder 实际推理不加载positional_embedding权重
+        def filt_weights(weights):
+            for item in weights:
+                if item[0] != "audio.positional_embedding":
+                    yield item
+
+        weights = filt_weights(weights)
+        loader = AutoWeightsLoader(self)
+        loader.load_weights(weights)
+
+    def compute_logits(
+            self,
+            hidden_states: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        return self.model.compute_logits(hidden_states, sampling_metadata)
+
+    def sample(
+            self,
+            logits: torch.Tensor,
+            sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        return self.model.sample(logits, sampling_metadata)
diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
index 33877829f..d78a60a0d 100644
--- a/vllm/model_executor/models/registry.py
+++ b/vllm/model_executor/models/registry.py
@@ -42,6 +42,7 @@ _TEXT_GENERATION_MODELS = {
     "BaichuanForCausalLM": ("baichuan", "BaichuanForCausalLM"),
     "BambaForCausalLM": ("bamba", "BambaForCausalLM"),
     "BloomForCausalLM": ("bloom", "BloomForCausalLM"),
+    "BailingMoeForCausalLM": ("bailing_moe", "BailingMoeForCausalLM"),
     "ChatGLMModel": ("chatglm", "ChatGLMForCausalLM"),
     "ChatGLMForConditionalGeneration": ("chatglm", "ChatGLMForCausalLM"),
     "CohereForCausalLM": ("commandr", "CohereForCausalLM"),
@@ -210,6 +211,7 @@ _MULTIMODAL_MODELS = {
     "Llama4ForConditionalGeneration": ("mllama4", "Llama4ForConditionalGeneration"),  # noqa: E501
     "SkyworkR1VChatModel": ("skyworkr1v", "SkyworkR1VChatModel"),
     "WhisperForConditionalGeneration": ("whisper", "WhisperForConditionalGeneration"),  # noqa: E501
+    "BailingMMNativeForConditionalGeneration": ("bailingmm", "BailingMMNativeForConditionalGeneration")  # noqa: E501
 }
 
 _SPECULATIVE_DECODING_MODELS = {
diff --git a/vllm/outputs.py b/vllm/outputs.py
index 65a6ed014..7202e92a2 100644
--- a/vllm/outputs.py
+++ b/vllm/outputs.py
@@ -118,6 +118,8 @@ class RequestOutput:
         encoder_prompt: Optional[str] = None,
         encoder_prompt_token_ids: Optional[list[int]] = None,
         num_cached_tokens: Optional[int] = None,
+        hidden_states: Optional[torch.Tensor] = None,
+        prefill_hidden_states: Optional[torch.Tensor] = None,
         *,
         multi_modal_placeholders: Optional[MultiModalPlaceholderDict] = None,
     ) -> None:
@@ -133,6 +135,8 @@ class RequestOutput:
         self.encoder_prompt = encoder_prompt
         self.encoder_prompt_token_ids = encoder_prompt_token_ids
         self.num_cached_tokens = num_cached_tokens
+        self.hidden_states = hidden_states
+        self.prefill_hidden_states = prefill_hidden_states
 
     def add(self, next_output: "RequestOutput", aggregate: bool) -> None:
         """Merge subsequent RequestOutput into this one"""
@@ -167,7 +171,9 @@ class RequestOutput:
     @classmethod
     def from_seq_group(
         cls, seq_group: SequenceGroup, use_cache: bool,
-        seq_id_to_seq_group: dict[str, SequenceGroupBase]
+        seq_id_to_seq_group: dict[str, SequenceGroupBase],
+        hidden_states: Optional[torch.Tensor] = None,
+        prefill_hidden_states: Optional[torch.Tensor] = None,
     ) -> Optional["RequestOutput"]:
         finished = seq_group.is_finished()
 
@@ -317,7 +323,9 @@ class RequestOutput:
             "encoder_prompt": encoder_prompt,
             "encoder_prompt_token_ids": encoder_prompt_token_ids,
             "num_cached_tokens": num_cached_tokens,
-            "multi_modal_placeholders": seq_group.multi_modal_placeholders
+            "multi_modal_placeholders": seq_group.multi_modal_placeholders,
+             "hidden_states": hidden_states,
+             "prefill_hidden_states": prefill_hidden_states,
         }
 
         if use_cache:
@@ -398,12 +406,16 @@ class RequestOutputFactory:
     @staticmethod
     def create(seq_group: SequenceGroup,
                seq_id_to_seq_group: dict[str, SequenceGroupBase],
-               use_cache: bool = False):
+               use_cache: bool = False,
+               hidden_states: Optional[torch.Tensor] = None,
+               prefill_hidden_states: Optional[torch.Tensor] = None):
         if seq_group.pooled_data is not None:
             return PoolingRequestOutput.from_seq_group(seq_group)
         else:
             return RequestOutput.from_seq_group(seq_group, use_cache,
-                                                seq_id_to_seq_group)
+                                                seq_id_to_seq_group,
+                                                hidden_states,
+                                                prefill_hidden_states)
 
 
 @dataclass
diff --git a/vllm/sampling_params.py b/vllm/sampling_params.py
index c430b74a9..f9ee15945 100644
--- a/vllm/sampling_params.py
+++ b/vllm/sampling_params.py
@@ -200,6 +200,8 @@ class SamplingParams(
         allowed_token_ids: If provided, the engine will construct a logits
             processor which only retains scores for the given token ids.
             Defaults to None.
+        return_hidden_states: If provided, hidden states of the last attention
+                    block are returned in the output.
         extra_args: Arbitrary additional args, that can be used by custom
             sampling implementations. Not used by any in-tree sampling
             implementations.
@@ -246,6 +248,7 @@ class SamplingParams(
     logit_bias: Optional[dict[int, float]] = None
     allowed_token_ids: Optional[list[int]] = None
     extra_args: Optional[dict[str, Any]] = None
+    return_hidden_states: Optional[bool] = None
 
     # Fields used for bad words
     bad_words: Optional[list[str]] = None
diff --git a/vllm/sequence.py b/vllm/sequence.py
index a97409523..fd3276487 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -1080,6 +1080,8 @@ class CompletionSequenceGroupOutput(
     # Prompt logprob for each prompt query token.
     prompt_logprobs: Optional[PromptLogprobs]
     step_index: Optional[int] = 0
+    hidden_states: Optional[torch.Tensor] = None
+    prefill_hidden_states: Optional[torch.Tensor] = None
 
     def __repr__(self) -> str:
         return (f"CompletionSequenceGroupOutput(samples={self.samples}, "
diff --git a/vllm/transformers_utils/config.py b/vllm/transformers_utils/config.py
index e062afd68..b3be79cf0 100644
--- a/vllm/transformers_utils/config.py
+++ b/vllm/transformers_utils/config.py
@@ -230,7 +230,7 @@ def _uses_mrope(config: PretrainedConfig) -> bool:
 
 def uses_mrope(config: PretrainedConfig) -> bool:
     """Detect if the model with this config uses M-ROPE."""
-    return _uses_mrope(config) or thinker_uses_mrope(config)
+    return _uses_mrope(config) or thinker_uses_mrope(config) or llm_uses_mrope(config)
 
 
 def thinker_uses_mrope(config: PretrainedConfig) -> bool:
@@ -245,6 +245,13 @@ def thinker_uses_mrope(config: PretrainedConfig) -> bool:
 
     return uses_mrope(thinker_text_config)
 
+def llm_uses_mrope(config: PretrainedConfig) -> bool:
+    llm_config = getattr(config, "llm_config", None)
+    if llm_config is None:
+        return False
+
+    return uses_mrope(llm_config)
+
 
 def is_encoder_decoder(config: PretrainedConfig) -> bool:
     """Detect if the model with this config is used as an encoder/decoder."""
@@ -761,12 +768,19 @@ def get_hf_text_config(config: PretrainedConfig):
     No op for pure text models.
     """
     # This block should be unnecessary after https://github.com/huggingface/transformers/pull/37517
+
     if hasattr(config, "thinker_config"):
         # TODO(suyang.fy): Refactor code.
         #  For Qwen2.5-Omni, change hf_text_config to
         #  thinker_config.text_config.
         return config.thinker_config.text_config
 
+    if hasattr(config, "llm_config"):
+        #  For Ming-lite, change hf_text_config to
+        #  llm_config.
+        return config.llm_config
+
+
     text_config = config.get_text_config()
 
     if text_config is not config:
diff --git a/vllm/transformers_utils/configs/__init__.py b/vllm/transformers_utils/configs/__init__.py
index 8812d4c48..5510489fc 100644
--- a/vllm/transformers_utils/configs/__init__.py
+++ b/vllm/transformers_utils/configs/__init__.py
@@ -25,6 +25,7 @@ from vllm.transformers_utils.configs.skyworkr1v import SkyworkR1VChatConfig
 from vllm.transformers_utils.configs.solar import SolarConfig
 from vllm.transformers_utils.configs.telechat2 import Telechat2Config
 from vllm.transformers_utils.configs.ultravox import UltravoxConfig
+from vllm.transformers_utils.configs.bailing_moe import BailingMoeConfig
 
 __all__ = [
     "ChatGLMConfig",
@@ -49,4 +50,5 @@ __all__ = [
     "SolarConfig",
     "Telechat2Config",
     "UltravoxConfig",
+    "BailingMoeConfig",
 ]
diff --git a/vllm/transformers_utils/configs/bailing_moe.py b/vllm/transformers_utils/configs/bailing_moe.py
new file mode 100644
index 000000000..4379368cf
--- /dev/null
+++ b/vllm/transformers_utils/configs/bailing_moe.py
@@ -0,0 +1,76 @@
+""" Bailing MoE model configuration """
+
+from transformers.configuration_utils import PretrainedConfig
+
+
+class BailingMoeConfig(PretrainedConfig):
+    model_type = "bailing_moe"
+
+    def __init__(
+        self,
+        vocab_size=30592,
+        hidden_size=1024,
+        intermediate_size=None,
+        num_hidden_layers=24,
+        num_attention_heads=16,
+        num_key_value_heads=0,
+        hidden_act="silu",
+        use_qkv_bias=False,  # bailing only
+        use_bias=True,  # bailing only
+        rms_norm_eps=1e-05,
+        norm_head=False,  # bailing only
+        tie_word_embeddings=False,  # PretrainedConfig key, here change default value.
+        embedding_dropout=0.1,
+        attention_dropout=0.1,
+        output_dropout=0.1,
+        initializer_range=0.02,
+        max_position_embeddings=16384,
+        rope_theta=10000.0,
+        use_cache=True,
+        use_sliding_window=False,
+        sliding_window=4096,
+        max_window_layers=28,
+        rope_scaling=None,
+        pad_token_id=126081,
+        num_experts=16,
+        num_shared_experts=0,
+        num_experts_per_tok=2,
+        norm_topk_prob=True,
+        moe_intermediate_size=None,
+        first_k_dense_replace=0,
+        head_dim=None,
+        **kwargs,
+    ):
+        self.num_hidden_layers = num_hidden_layers
+        self.vocab_size = vocab_size
+        self.hidden_size = hidden_size
+        self.intermediate_size = intermediate_size
+        self.num_attention_heads = num_attention_heads
+        self.num_key_value_heads = num_key_value_heads
+        self.hidden_act = hidden_act
+        self.use_qkv_bias = use_qkv_bias
+        self.use_bias = use_bias
+        self.norm_head = norm_head
+        self.rms_norm_eps = rms_norm_eps
+        self.embedding_dropout = embedding_dropout
+        self.attention_dropout = attention_dropout
+        self.output_dropout = output_dropout
+        self.initializer_range = initializer_range
+        self.max_position_embeddings = max_position_embeddings
+        self.rope_theta = rope_theta
+        self.use_cache = use_cache
+        self.use_sliding_window = use_sliding_window
+        self.sliding_window = sliding_window
+        self.max_window_layers = max_window_layers
+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads
+        self.rope_scaling = rope_scaling
+
+        # MoE configs
+        self.num_experts = num_experts
+        self.num_shared_experts = num_shared_experts
+        self.num_experts_per_tok = num_experts_per_tok
+        self.norm_topk_prob = norm_topk_prob
+        self.moe_intermediate_size = moe_intermediate_size
+        self.first_k_dense_replace = first_k_dense_replace
+
+        super().__init__(pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs)
diff --git a/vllm/v1/executor/multiproc_executor.py b/vllm/v1/executor/multiproc_executor.py
index cb125bf4b..2bd8b25c5 100644
--- a/vllm/v1/executor/multiproc_executor.py
+++ b/vllm/v1/executor/multiproc_executor.py
@@ -225,9 +225,10 @@ class MultiprocExecutor(Executor):
         if not getattr(self, 'shutting_down', False):
             self.shutting_down = True
             self.shutdown_event.set()
-            for w in self.workers:
-                w.worker_response_mq = None
-            self._ensure_worker_termination([w.proc for w in self.workers])
+            if workers := getattr(self, 'workers', None):
+                for w in workers:
+                    w.worker_response_mq = None
+                self._ensure_worker_termination([w.proc for w in workers])
 
         self.rpc_broadcast_mq = None
 
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 73e0eff9a..8acf29013 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -685,8 +685,32 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
 
         inter_data.multi_modal_kwargs = mm_kwargs
         inter_data.multi_modal_placeholder_maps = placeholder_maps
-
         # special processing for mrope position deltas.
+        if hasattr(self.runner.model_config.hf_config, "llm_config"):
+            if self.runner.model_config.hf_config.llm_config.rope_scaling["use_3d_rope"]:
+                image_grid_thw = mm_kwargs.get("image_grid_thw", None)
+                video_grid_thw = mm_kwargs.get("video_grid_thw", None)
+                audio_feature_lengths = mm_kwargs.get("audio_feat_lengths", None)
+                hf_config = self.runner.model_config.hf_config
+                inter_data.mrope_input_positions = [None] * inter_data.n_seqs
+                for seq_idx in range(inter_data.n_seqs):
+                    seq_data = seq_group_metadata.seq_data[
+                        inter_data.seq_ids[seq_idx]]
+                    token_ids = seq_data.get_token_ids()
+                mrope_input_positions, mrope_position_delta = \
+                    MRotaryEmbedding.get_input_positions(
+                        token_ids,
+                        hf_config=hf_config,
+                        image_grid_thw=image_grid_thw,
+                        video_grid_thw=video_grid_thw,
+                        second_per_grid_ts=0,
+                        context_len=inter_data.context_lens[seq_idx],
+                        seq_len=inter_data.seq_lens[seq_idx],
+                    )
+                seq_data.mrope_position_delta = mrope_position_delta
+                inter_data.mrope_input_positions[seq_idx] = mrope_input_positions
+                return
+
         if self.runner.model_config.uses_mrope:
             image_grid_thw = mm_kwargs.get("image_grid_thw", None)
             video_grid_thw = mm_kwargs.get("video_grid_thw", None)
@@ -1708,6 +1732,12 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
         # virtual engines share the same kv cache.
         virtual_engine = model_input.virtual_engine
         previous_hidden_states = kwargs.get("previous_hidden_states")
+        if (model_input.sampling_metadata is not None
+                and hasattr(model_input.sampling_metadata, 'seq_groups')
+                and model_input.sampling_metadata.seq_groups is not None):
+            self.return_hidden_states = (
+                model_input.sampling_metadata.seq_groups[0].sampling_params.
+                return_hidden_states)
         if prefill_meta is None and decode_meta.use_cuda_graph:
             assert model_input.input_tokens is not None
             graph_batch_size = model_input.input_tokens.shape[0]
-- 
2.19.1.6.gb485710b

